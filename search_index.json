[["index.html", "Close-kin mark-recapture: theory and practice — Spokane, Washington, USA Course Overview Workshop schedule Resources Setting up your computer A word about the ‘renv’ package A word about the TMB package", " Close-kin mark-recapture: theory and practice — Spokane, Washington, USA Paul B. Conn and Eric C. Anderson 2022-10-26 Course Overview This is the website/book associated with the Close-kin mark-recapture: theory and practice workshop to be held at The Wildlife Society meetings Nov 6, 2022 in Spokane, Washington. Workshop schedule An approximate schedule for the workshop is as follows. Note that when a session has slides for a lecture, they are linked below. Time Description Instructor(s) 8:00–8:45 Close-kin mark-recapture: An overview (slides) P. Conn 8:45–9:30 An introduction to genetic data and inheritance (slides) E. Anderson 9:30–9:45 Break 9:45–10:30 Statistical inference for CKMR estimation P. Conn 10:30–11:15 Kin finding E. Anderson 11:15–12:00 Designing a CKMR experiment P. Conn 12:00–1:00 Lunch 1:00–5:00 R/TMB labs (full day participants only) P. Conn/E. Anderson Workshop attendees had the option of registering for half-day or full-day sessions. The first half will consist almost entirely of lectures, with no previous programming experience needed. The second half will involve labs and exercises conducted in R and/or Template Model Builder (at the user’s discretion). Full day participants should use the following set of instructions to set up their computers prior to the workshop. Please do this within no more than a day or two of the workshop, as the materials will be evolving up to the day of launch! (Additionally, if you follow the instructions below, any new changes to the afternoon lab materials will be fast and easy to get). Resources All the materials for the workshop, including this “book” and all of the slides are located on github at https://github.com/eriqande/tws-ckmr-2022 The “website” version of the course which serves up this book and the slides is available at https://eriqande.github.io/tws-ckmr-2022/ These course materials will remain publicly available in perpetuity at the above addresses. We might occasionally update them as well. There is also additional material and examples that may be of interest at https://closekin.github.io/ Setting up your computer This is only really necessary if you are signed up for the full day course This course covers topics in close-kin mark recapture, and the second half of the course relies heavily on the R programming language. In order to follow along with the code and be successful in running all of the examples, it will be helpful (if not imperative) for full-day workshop participants to have recent versions of R and RStudio. Step 1. Install recent versions of R and RStudio The workshop materials were developed using: R versions 4.1 and 4.2 Rstudio Version &gt;= 2022.07.01 If you have an older version of either of those applications, you should install the latest. Important Several of the packages must be compiled locally on your laptop, and the TMB package operates by compiling C++ code. Accordingly, in addition to installing R, and RStudio, you must have a compile/build chain on your computer: On a Mac, you need to do sudo xcode-select --install as described here. Note that this requires admin privileges, so, if your computer is administered by your agency or university IT department, get this done before arriving in Spokane. For Windows, you must download and install the Rtools as available here. We have tested the workshop materials on several platforms: a Mac with an Intel chip running BigSur OSX 11 a Mac with an M1 (Apple silicon) chip running Monterey version 12.6 a PC running Windows 10. We suspect it will also work on most other Mac or Windows operating systems. Step 2. Make sure that you have git installed on your system If you are on a Mac and successfully did sudo xcode-select --install, then you will have git. If you are on a PC and you do not have git, then follow the instructions in the excellent (HappyGitWithR)[https://happygitwithr.com/] book, specifically the “highly recommended” option 1. A further word on git Although the distribution our workshop materials depends heavily on git, and the cloud-based code management system GitHub, that is built upon git, we won’t have the time to delve deeply into these topics. You can do everything you need to do for this workshop without having an account on GitHun, but, if you are interested in version control for your analyses, and you are interested in using GitHub to share and present the results of your research, then you really will want to become proficient with both git and GitHub. Once again, the online book https://happygitwithr.com/ is highly recommended. Step 3. Get the workshop materials from GitHub with RStudio Once git is installed and RStudio knows where to find it, RStudio can use it to download the workshop materials from GitHub. Follow these steps: Open RStudio From the dropdown menus, choose File -&gt; RStudio In the resulting pop-up window choose and click “Version Control”: In the next screen of the “wizard” choose “Git”: In the next screen of the wizard, paste the following URL into the Repository URL box. https://github.com/eriqande/tws-ckmr-2022.git Then use the “Browse” button to tell where you want the project to be stored on your computer (that is up to you!), click “Open in new session” (why not—no reason to close out whatever you were working on in another RStudio project), and then click “Create Project”: RStudio then downloads the project and opens it for you. It also detects that the project is using the ‘renv’ package (see below) and tells you how to get all the packages you need. It should look something like this: . To get all the packages, paste these lines into your R console in RStudio: install.packages(&quot;rmarkdown&quot;) renv::restore() That should start with ‘renv’ giving you a list of all the packages that will be installed or updated in the repository-local library tree (this should not change versions of packages in your own use-wide or system-wide collection of packages), and it will ask something like: Do you want to proceed? [y/N]: Enter y. That should launch what could be a fairly lengthy process of downloading, compiling (a few, at least) and installing all the R packages we need, and their dependencies. That should do it, and if it works for most everyone in the course with relatively few hitches, then it is a testament to how well the ‘renv’ package is implemented. If there are some hiccups, we will try our best to help you deal with them. 0.0.1 Updating the project It is possible that the course instructors may update the project repository with new information, code, or content after you have followed the steps above to do the initial installation. Fortunately, it is easy to get those updates from GitHub. Updating your project can be done in these two easy steps: Pull the changes down from GitHub. In RStudio, this means hitting the “Pull” button in the Git panel: Then in the R console, execute the command. renv::restore() That’s it! A word about the ‘renv’ package Our work will require a number of packages that can be found in binary form on CRAN. As well as several that require compilation from source on GitHub. This is why the compile/build chain (see above) is essential. This year, we are using the ‘renv’ package to help with installing the necessary packages in an isolated R library on your computer. The ‘renv’ package guides the installation of all the packages (with specific version numbers that we have tested for this workshop) into a local R library associated with the workshop’s RStudio project. This means that installing all these new packages will not overwrite your current R library tree. So, nothing that we do should change the way your current R setup works in other projects (fingers crossed!). A word about the TMB package In the full day workshop we will make some use of Template Model Builder (TMB); attendees might wish to familiarize themselves with how it works by reading some of its documentation, such as that at https://kaskr.github.io/adcomp/Introduction.html). For the mathematically or computationally inclined, the article about TMB on arXiv might be a fun read. "],["ckmr-thought-experiments-and-labs.html", "Session 1 CKMR thought experiments and labs 1.1 Afternoon schedule 1.2 Thought experiments", " Session 1 CKMR thought experiments and labs In the second half of this course, we’re going to switch gears and delve into some code. But first, we’re going to conduct a few thought experiments to try to get you to think critically about some of the CKMR concepts we’ve covered so far. We’ll also be switching from slides to this “book” - hopefully this will be a useful resource for those of you who want to look back at course content. 1.1 Afternoon schedule An approximate schedule for the afternoon is 1:00 - 1:30 Thought experiments 1:30 - 2:30 Kin finding lab 2:30 - 2:45 Break 2:45 - 3:30 White shark estimation example 3:30 - 4:30 CKMR Design 4:30 - 5:00 Open! Feel free to play with code or ask us questions 1.2 Thought experiments We think it would be useful for participants to break into small groups for this one. So let’s do that! 1.2.1 Thought experiment 1 "],["simulation-and-inference-play-with-hillary-et-al.s-white-shark-ckmr-example.html", "Session 2 Simulation and Inference Play with Hillary et al.’s White Shark CKMR example 2.1 Simulating from their inferential model 2.2 An R function to compute the negative log-likelihood 2.3 Evaluate the likelihood with TMB 2.4 Visualize those log likelihood values 2.5 OMG! Let’s investigate the tmbstan package!", " Session 2 Simulation and Inference Play with Hillary et al.’s White Shark CKMR example Here we explore the models used in Hillary et al. (2018) to estimate abundance of white sharks in Australia. Looking through the paper, I didn’t find any direct mention of a public repository of data that was used in the paper, nor any repository of code for their analyses. So, we are just going to simulate some data here from their model (first just from their inference model, and maybe later we will do so with a simple individual-based-model) so that we have something to work with. One nice thing about simulating from their inference model is that doing so offers a good way to gain a better understanding their inference model. 2.1 Simulating from their inferential model They start with a demographic model of exponential growth: \\[ N_t^A = N_\\mathrm{init}e^{\\lambda t} \\] where \\(N_t^A\\) is the number of adults at time \\(t\\) and \\(N_\\mathrm{init}\\) is the initial number of adults (at time \\(t = 0\\)). \\(\\lambda\\) is an exponential growth rate. Here, we define a function to calculate \\(N_t^A\\): library(tidyverse) library(microbenchmark) library(TMB) #&#39; calculate the number of adults at time t, given exponential growth or decline #&#39; @param N0 number of adults at time 0 #&#39; @param t time #&#39; @param lambda exponential growth rate parameter N_adults &lt;- function(N0, t, lambda) { N0 * exp(lambda * t) } Let’s see what that would look like over 60 years, starting from a few different \\(N_\\mathrm{init}\\) values and growing/shrinking at a range of values. A_traj &lt;- expand_grid( Ninit = c(600, 800, 1000, 1200, 1500, 2000), lambda = seq(-0.03, 0.03, by = 0.01) )%&gt;% mutate( num_adults = map2(Ninit, lambda, function(x, y) N_adults(x, 0:60, y)), ) %&gt;% unnest(num_adults) %&gt;% group_by(Ninit, lambda) %&gt;% mutate(year = 0:60) ggplot(A_traj, aes(x = year, y = num_adults, colour = factor(lambda))) + geom_line() + facet_wrap(~ Ninit) Now we are going to write a function to simulate sampled pairs from such a population, and then we will simulate some of them to be half-siblings according to the probabilities in their inferential model. #&#39; simulate sampling n sharks a year from year Slo to Shi and return all the sample pairs #&#39; @param npy number of sharks to sample each year (num per year) #&#39; @param Slo year to start sampling #&#39; @param Shi final year in which to sample #&#39; @param ages a vector of the ages at which individuals are sampled #&#39; @param age_wts vector of expected proportion of each age in the sample #&#39; @param N0 initial population size #&#39; @param lambda population growth rate #&#39; @param Thi final time to simulate an abundance (must be &gt;= than Shi) #&#39; @param phiA Adult annual survival probability #&#39; @return This returns a list of three components: `pairs`: a tibble of all the pairs, #&#39; `pair_counts`: a tibble of counts of different pair types of different age differences, #&#39; and `samples`: a simple tibble of just the samples, and `N`: a tibble of the #&#39; number of adults each year. simulate_pairs &lt;- function( npy = 10, Slo = 20, Shi = 40, ages = 3:8, age_wts = rep(1, length(ages)), N0 = 800, lambda = 0.01, Thi = 60, phiA = 0.94 ) { A_traj &lt;- tibble( year = 0:Thi ) %&gt;% mutate( num_adults = N_adults(N0, year, lambda) ) samples &lt;- tibble( samp_year = rep(Slo:Shi, each=npy) ) %&gt;% mutate( age = sample(ages, size = n(), replace = TRUE, prob = age_wts), born_year = samp_year - age ) %&gt;% left_join(A_traj, by = c(&quot;born_year&quot; = &quot;year&quot;)) n &lt;- nrow(samples) s1 &lt;- samples[rep(1:n, each = n),] s2 &lt;- samples[rep(1:n, times = n),] names(s2) &lt;- paste0(names(s2), &quot;.old&quot;) pairs &lt;- bind_cols(s1, s2) %&gt;% filter(born_year.old &lt; born_year) %&gt;% mutate( age_diff = born_year - born_year.old, HSP_prob = (4/num_adults) * (phiA ^ (born_year - born_year.old)), isHSP = rbernoulli(n(), p = HSP_prob) ) pair_counts &lt;- pairs %&gt;% count(born_year, age_diff, HSP_prob, isHSP) %&gt;% pivot_wider(names_from = isHSP, values_from = n, values_fill = 0L) %&gt;% rename(n_UP = `FALSE`, n_HSP = `TRUE`) list( pairs = pairs, pair_counts = pair_counts, pair_data = pair_counts %&gt;% select(-HSP_prob), samples = samples, N = A_traj ) } Now that we have done that, let’s simulate some pairs. We will assume that 20 sharks were sampled per year—that is more than Hillary et al. (2018) had, I think, but it makes the likelihood surface a little cleaner, so, since we can simulate whatever we want, we will simulate an abundance of data: set.seed(5) sim_vals &lt;- simulate_pairs(npy = 20, age_wts = 8:3) At this juncture, it is worth stopping for a moment and looking at two of the elements of the list sim_vals. The pair_counts show the number of half-sibling pairs of different types, and also shows the half-sibling pair probability that was used to simulate them: sim_vals$pair_counts ## # A tibble: 325 × 5 ## born_year age_diff HSP_prob n_UP n_HSP ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 13 1 0.00413 10 0 ## 2 14 1 0.00409 35 0 ## 3 14 2 0.00384 14 0 ## 4 15 1 0.00405 84 0 ## 5 15 2 0.00380 60 0 ## 6 15 3 0.00357 24 0 ## 7 16 1 0.00401 216 0 ## 8 16 2 0.00376 124 2 ## 9 16 3 0.00354 90 0 ## 10 16 4 0.00333 36 0 ## # … with 315 more rows ## # ℹ Use `print(n = ...)` to see more rows In this tibble, n_HSP is the number of half-sibling pairs found in which the younger member was born in born_year and the older member was born age_diff years before. As you can see, there are a lot of categories for which no HSPs are found. That is to be expected—they are pretty rare, and, of course, you expect to see fewer of them if the population is large…that is how CKMR works… The HSP_prob column gives the probability of seeing an HSP of a certain category given the values of the parameters that were used to simulate the data. Typically if you had some observed data, you wouldn’t compute the HSP_prob for parameter values you didn’t know. So, if you had observed data it would look like: sim_vals$pair_data ## # A tibble: 325 × 4 ## born_year age_diff n_UP n_HSP ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 13 1 10 0 ## 2 14 1 35 0 ## 3 14 2 14 0 ## 4 15 1 84 0 ## 5 15 2 60 0 ## 6 15 3 24 0 ## 7 16 1 216 0 ## 8 16 2 124 2 ## 9 16 3 90 0 ## 10 16 4 36 0 ## # … with 315 more rows ## # ℹ Use `print(n = ...)` to see more rows And the goal from those data would be to estimate the parameters that might have produced these data. It is, however, worthwhile to plot the half-sibling pair probabilities, just to look at those. sim_vals$pairs %&gt;% group_by(born_year, age_diff) %&gt;% summarise(HSP_prob = mean(HSP_prob)) %&gt;% ggplot(aes(x = born_year, y = HSP_prob, fill = factor(age_diff))) + geom_point(shape = 21) + xlab(&quot;Year the younger member of pair was born&quot;) + ylab(&quot;Half-sibling probability&quot;) ## `summarise()` has grouped output by &#39;born_year&#39;. You ## can override using the `.groups` argument. 2.2 An R function to compute the negative log-likelihood To estimate that parameters that gave rise to the simulated data, we can start with an R function to compute the negative log-likelihood—in other words, the probability of our observed data given values of the parameters, which are the initial population size \\(N_\\mathrm{init}\\), the population growth rate \\(\\lambda\\), and the probability that an adult survives for a year, \\(\\phi_A\\). The following R function computes that CKMR pseudolikelihood: #&#39; Return the negative log likelihood of the parameter values given the data #&#39; @param pars a vector (Ninit, lambda, phiA) #&#39; @param X a tibble like the pair_counts component of the output list from #&#39; `simulate_pairs()`. hsp_nll &lt;- function(pars, X) { N0 &lt;- pars[1] L &lt;- pars[2] P &lt;- pars[3] LL &lt;- X %&gt;% mutate( N = N0 * exp(L * born_year), hspp = (4 / N) * P ^ age_diff, logl = log(hspp) * n_HSP + log(1 - hspp) * n_UP ) -sum(LL$logl) } Let’s first compute the negative log-likelihood for the values used in the simulation: hsp_nll( pars = c(800, 0.01, 0.94), X = sim_vals$pair_data ) ## [1] 1508 Since this is a negative log likelihood, a smaller number is better. We can see that if we choose parameter values that are far from the true ones we get a bigger (less good) value of the negative log likelihood. hsp_nll( pars = c(1800, 0.05, 0.94), X = sim_vals$pair_data ) ## [1] 1752 It is a fun exercise to visualize this log-likelihood surface. But we are not going to do that with this hsp_nll function, because it is quite slow. Observe how many milliseconds it takes to evaluate the likelihood using the hsp_nll() function: mb &lt;- microbenchmark::microbenchmark( R_version = hsp_nll( pars = c(1800, 0.05, 0.94), X = sim_vals$pair_data ), times = 100 ) # on average, typically more than 2 milliseconds: mb ## Unit: milliseconds ## expr min lq mean median uq max neval ## R_version 2.716 2.898 3.019 2.991 3.09 3.795 100 In general, straight-up R is a little underpowered for computing these sorts of likelihoods, and there are some serious advantages to calculating them in compiled code using the package TMB. We will do that next. 2.3 Evaluate the likelihood with TMB You need the following code in a file TMB/hsp_nll.cpp, relative to the current working directory. (That file is already in this repository). // simple half-sibling CKMR with exponential pop growth and known ages #include &lt;TMB.hpp&gt; template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { DATA_VECTOR(n_UP); DATA_VECTOR(n_HSP); DATA_VECTOR(born_year); DATA_VECTOR(age_diff); PARAMETER(N_init); PARAMETER(lambda); PARAMETER(phiA); ADREPORT(N_init); ADREPORT(lambda); ADREPORT(phiA); Type N; // for storing the pop size at time of younger kids birth Type hsp; // for storing the half-sibling pair prob Type nll = 0; for(int i=0;i&lt;n_UP.size(); i++) { N = N_init * exp(lambda * born_year[i]); hsp = (4 / N) * pow(phiA, age_diff[i]); nll -= log(hsp) * n_HSP[i] + log(1 - hsp) * n_UP[i]; } return nll; } That file first must be compiled. compile(&quot;TMB/hsp_nll.cpp&quot;) ## [1] 0 After that we will make some data and parameter lists: # then get our data in a list of vectors data &lt;- sim_vals$pair_data %&gt;% select(n_UP, n_HSP, born_year, age_diff) %&gt;% as.list() # and then our starting parameters parameters &lt;- list(N_init = 1000, lambda = 0.001, phiA = 0.9) Now, to make an AD function with TMB, we need to link to the library created when we compiled the file above. For unknown reasons,on a Mac, TMB seems unable to deal with having the compiled object files in a subdirectory, so, in the following code chunk we test to see if we are on a Mac, and if we are we momentarily switch to the TMB directory. Otherwise we just run the functions the way we should be able to. if(Sys.info()[&quot;sysname&quot;] == &quot;Darwin&quot;) { setwd(&quot;TMB&quot;) dyn.load(dynlib(&#39;hsp_nll&#39;)) obj &lt;- TMB::MakeADFun(data = data, parameters = parameters, DLL=&quot;hsp_nll&quot;) setwd(&quot;..&quot;) } else { dyn.load(dynlib(&quot;TMB/hsp_nll&quot;)) obj &lt;- TMB::MakeADFun(data = data, parameters = parameters, DLL=&quot;hsp_nll&quot;) } Now, we might want to see how long it takes for this compiled version of the HSP negative log likelihood to be computed: mb2 &lt;- microbenchmark( TMB_version = obj$fn(x = parameters), times = 1000 ) # this is in microseconds, so it is effectively 100 times faster # to evalaute this using TMB. mb2 ## Unit: microseconds ## expr min lq mean median uq max neval ## TMB_version 32.7 33.49 65 33.75 34.05 7564 1000 2.4 Visualize those log likelihood values Now that we have a fast way to evaluate those, using TMB, let’s evaluate a lot of log likelihoods so we can make some contour plots. We will evaluate the log likelihood over a range of values of the three parameters: LL_tib &lt;- expand_grid( N_init = seq(200, 2000, by = 20), lambda = seq(-0.04, 0.04, by = 0.01), phiA = seq(0.85, 1, by = 0.01) ) %&gt;% mutate(parameters = pmap( .l = list(a = N_init, b = lambda, c= phiA), .f = function(a,b,c) list(N_init = a, lambda = b, phiA = c) )) %&gt;% mutate( nll = map_dbl( .x = parameters, .f = function(y) obj$fn(x = y) ) ) With all those results, we can make a contour plot, faceted over lambda values to visualize: LL_tib %&gt;% group_by(lambda) %&gt;% mutate(nll = ifelse(nll &gt; min(nll) + 20, NA, nll)) %&gt;% ungroup() %&gt;% ggplot(aes(x = N_init, y = phiA, z = nll)) + geom_contour(binwidth = 1, colour = &quot;gray&quot;) + theme(legend.position = &quot;none&quot;) + facet_wrap(~lambda) + theme_bw() + geom_vline(xintercept = 800, colour = &quot;blue&quot;) + geom_hline(yintercept = 0.94, colour = &quot;red&quot;) ## Warning: Removed 9477 rows containing non-finite values ## (stat_contour). I am sure there is a cleaner way of dropping all the contour lines smaller than a certain value, but I am not going to bother with it here. The blue and red lines show where the actual “true” simulated values of \\(N_\\mathrm{init}\\) and \\(\\phi_A\\) are, respectively. Recall that the true value of \\(\\lambda\\) is 0.01. And see on that facet that the true values of \\(N_\\mathrm{init}\\) and \\(\\phi_A\\) are near the top of the likelihood (or the bottom of the negative log likelihood, if you prefer to think of it that way). We see that with this much data, and whilst assuming the correct growth rate, the abundance and the adult survival are estimated quite accurately. The question remains, however, of how much information there is about \\(\\lambda\\). Can we estimate that well? 2.5 OMG! Let’s investigate the tmbstan package! One very nice feature of using TMB is that we can plug our TMB object directly into Stan for doing Bayesian inference via No-U-turn MCMC sampling. We do this with the ‘tmbstan’ package. Awesome! Note that we sort of need to impose some bounds on the parameters. Otherwise, as one might expect seeing the contour plots of the likelihood surfaces, we can get estimated survival parameters exceeding 1.0, which just doesn’t make sense. This is one particularly nice feature of using the TMB object in a Bayesian context—it seems easier to impose these sorts of parameter bounds, than when using the TMB object to maximize the likelihood. For some reason that I don’t full understand, we have to remake the TMB AD function before we run the following. Something about having used obj$fn to compute the values of the Neg Log Likelihood, in order to make our pretty contour plots, has gummed things up. But no worries, we just define it again: setwd(&quot;TMB&quot;) dyn.load(dynlib(&#39;hsp_nll&#39;)) obj &lt;- MakeADFun(data = data, parameters = parameters, DLL=&quot;hsp_nll&quot;) setwd(&quot;..&quot;) Now we can hand that to tmbstan and do a short MCMC run: library(tmbstan) ## Loading required package: rstan ## Loading required package: StanHeaders ## rstan (Version 2.21.5, GitRev: 2e1f913d3ca3) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) ## ## Attaching package: &#39;rstan&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract set.seed(10) fit &lt;- tmbstan( obj, chains=1, lower = c(N_init = 20, lambda = -0.5, phiA = 0.2), upper = c(N_init = Inf, lambda = 0.5, phiA = 1.0), iter = 7500 ) ## ## SAMPLING FOR MODEL &#39;tmb_generic&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000106 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.06 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 7500 [ 0%] (Warmup) ## Chain 1: Iteration: 750 / 7500 [ 10%] (Warmup) ## Chain 1: Iteration: 1500 / 7500 [ 20%] (Warmup) ## Chain 1: Iteration: 2250 / 7500 [ 30%] (Warmup) ## Chain 1: Iteration: 3000 / 7500 [ 40%] (Warmup) ## Chain 1: Iteration: 3750 / 7500 [ 50%] (Warmup) ## Chain 1: Iteration: 3751 / 7500 [ 50%] (Sampling) ## Chain 1: Iteration: 4500 / 7500 [ 60%] (Sampling) ## Chain 1: Iteration: 5250 / 7500 [ 70%] (Sampling) ## Chain 1: Iteration: 6000 / 7500 [ 80%] (Sampling) ## Chain 1: Iteration: 6750 / 7500 [ 90%] (Sampling) ## Chain 1: Iteration: 7500 / 7500 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 3.89677 seconds (Warm-up) ## Chain 1: 4.26565 seconds (Sampling) ## Chain 1: 8.16243 seconds (Total) ## Chain 1: # and here is a summary of the result: fit ## Inference for Stan model: hsp_nll. ## 1 chains, each with iter=7500; warmup=3750; thin=1; ## post-warmup draws per chain=3750, total post-warmup draws=3750. ## ## mean se_mean sd 2.5% 25% ## N_init 915.51 10.08 351.90 416.03 668.15 ## lambda 0.00 0.00 0.01 -0.02 -0.01 ## phiA 0.93 0.00 0.02 0.89 0.91 ## lp__ -1505.94 0.04 1.29 -1509.39 -1506.50 ## 50% 75% 97.5% n_eff Rhat ## N_init 860.22 1079.69 1780.87 1218 1 ## lambda 0.00 0.01 0.03 1209 1 ## phiA 0.92 0.94 0.96 1324 1 ## lp__ -1505.61 -1505.00 -1504.50 954 1 ## ## Samples were drawn using NUTS(diag_e) at Fri Oct 21 19:00:41 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Anyone who has spent months trying to “roll their own” MCMC sampler will recognize that this was uncharacteristically painless. That is part of the joy of being able to calculate gradients (with the auto-differentiation features of TMB) to use those in Hamiltonian Monte Carlo. Also, the maintainers of Stan, TMB, and tmbstan have done an amazing job! If you want a really slick visual representation of the results from the MCMC, you can do: library(shinystan) launch_shinystan(fit) We don’t do that in this Rmarkown document, because you need to be in a an interactive session for it to work, but, just know that it pops up a shiny window like this: Also, if we wanted to start our chain from different starting values and run it in parallel on different cores, we can do that like this, though it seems the interface to this has changed a little. Try ?tmbstan for details. cores &lt;- parallel::detectCores()-1 options(mc.cores = cores) set.seed(1) init.list &lt;- lapply(1:cores, function(x) { c( N_init = runif(1, min = 2000, max = 20000), lambda = runif(1, -0.04, 0.04), phiA = runif(1, 0.75, 0.999) )} ) fit2 &lt;- tmbstan( obj, chains=cores, open_progress=FALSE, init=init.fn, lower = c(N_init = 20, lambda = -0.5, phiA = 0.2), upper = c(N_init = Inf, lambda = 0.5, phiA = 1.0), iter = 6000, warmup = 2000 ) That is remarkably easy and slick. I’m impressed. References "],["design-of-ckmr-experiments.html", "Session 3 Design of CKMR Experiments 3.1 White shark design example 3.2 Bearded seal simulation study", " Session 3 Design of CKMR Experiments In this lab, we’ll look at various tools to help design CKMR experiments. We’re relatively liberal with what we call a “design” tool. It could be optimizing sample allocation to achieve maximum precision for a given parameter (e.g., which sexes and ages to target), or it could simply be examining the effect of different types of assumption violations on estimator performance. After all, if a particular type of model is likely to lead to highly biased parameter estimates, it is probably best to adapt model structure, or to leave out certain types of kin comparisons that are likely to violate assumptions. We’ll consider two specific examples: (1) optimizing sample allocation in Hillary et al. (2018) white shark example, and (2) conducting individual-based simulation to investigate impacts of assumption violations in CKMR models for bearded seals. 3.1 White shark design example In a previous lab, we looked at Hillary et al. (2018) model using cross-cohort comparisons of half siblings caught as juveniles. We might look at a few things with this example, such as how we would partition samples across years and ages if we had the ability to do so. Presumably, the optimal allocation would depend on the parameter we were interested in (e.g., population growth (\\(\\lambda\\)), adult survival (\\(S\\)) or terminal year abundance (call this \\(N_T\\)). Both \\(\\lambda\\) and \\(S\\) are parameters of the CKMR model, but \\(N_T\\) is a derived parameter (that is, a function of parameters) and can be computed as \\[\\begin{equation*} N_T = N_0 * \\exp(\\lambda T) \\end{equation*}\\] where \\(T\\) is the number of years in the study. 3.1.1 Fisher information ideas We’ll be using Fisher information ideas to compare anticipated precision associated with various white shark sampling designs. As articulated in the slides on design for this workshop, this involves a number of steps: Allocate potential sample sizes to particular years, ages, sexes, etc. (basically the different covariates one is modeling). These can be expectations (fractions)! Compute sufficient statistics (expected # of comparisons and number of matches for each covariate combination) Treating these as data, calculate second derivatives of the negative log-pseudo-likelihood at the true parameter values Calculate the expected variance-covariance matrix of parameters in the usual way (inverse of the numeric Fisher information matrix) Calculate expected variance of any functions of parameters using the delta method Compare precision (e.g. CV) associated with different designs!! Fortunately, we already have the infrastructure set up to help us the negative log-pseudo-likelihood (both in R and TMB). We could use numerical second derivatives (e.g., using the \\(numDeriv\\) package in R) and not even have to deal with minimizing the NLPL, but Template Model Builder is well set up to produce numerical variance-covariance matrices (including functions of parameters), so why don’t we just follow the path of least resistance and fit our model using TMB. We’ll still need to formulate some alternative designs and what the expected data would look like under each. 3.1.2 Calculating expected data under different design scenarios Let’s set up “true” population dynamics to consist of a stable population with 1000 adults which we assume to be constant over a 20 year period. We’ll set the assume a constant adult survival probability of 0.94, and that sampling occurs over the last 10 years of the time interval. We’ll assume that sampling targets juveniles (ages 3-8) only, so the most we’ll have to go back in time is 8 years (which is why the model needs to go back further in time than just the years that are sampled!!). Ninit = 1000 lambda = 0.0 n_yrs = 20 N = matrix(Ninit,20) phiA = 0.94 ylo=11 yhi=20 ages = c(3:8) #only 3 - 8 yrs olds sampled #&#39; Function to calculate # of expected pairs given true adult abundance, adult #&#39; survival, and # of juvenile white sharks sampled per year (males and female parents modeled together!) #&#39; @param Npy vector of number of sharks to sample each year (num per year) #&#39; @param ages a vector of the ages at which individuals are sampled #&#39; @param age_wts vector of expected proportion of each age in the sample #&#39; @param N Vector of number of adults per year #&#39; @param phiA Adult annual survival probability #&#39; @return This returns two upper triangular matrices `M` and `EC`, both of which have dimension #&#39; (n_yrs x #&#39; n_yrs). The M matrix holds the number of comparisons where the row indexes the #&#39; birth year of the older #&#39; half-sib, and the column gives the birth year of the younger #&#39; half sib. The EC matrix is organized the same way, but holds the expected half-sib count #&#39; (i.e. # of matches) expected_data &lt;- function(Npy,ylo,yhi,ages,age_wts,N,phiA){ age_wts = age_wts/sum(age_wts) #normalize if not n_ages = max(ages)+1 #nb this is just age 0 to the max of sampled age age_prob = rep(0,n_ages) age_prob[ages+1]=age_wts/sum(age_wts) n_yrs = length(Npy) M = EC = matrix(0,n_yrs,n_yrs) # expected number sampled by year and age N_samp = matrix(0,n_yrs,max(ages)+1) #this holds expected number sampled by year and age (w/ age 0) for(iyr in 1:n_yrs){ N_samp[iyr,]=Npy[iyr]*age_prob } # convert to number sampled by birth year N_samp_by = rep(0,n_yrs) for(iyr in 11:n_yrs){ #this would need to be changed to make general - i.e., sampling occurred &lt; year 10 for(iage in 1:n_ages){ N_samp_by[iyr-iage+1]=N_samp_by[iyr-iage+1]+N_samp[iyr,iage] } } #Number of comparisons, probability of matches, expected number of matches for(iyr in 1:(n_yrs-1)){ for(iyr2 in (iyr+1):n_yrs){ M[iyr,iyr2]=N_samp_by[iyr]*N_samp_by[iyr2] age_diff = iyr2-iyr HSP_prob = 4/N[iyr]*(phiA^age_diff) #nb: there&#39;s some duplication here! EC[iyr,iyr2]=M[iyr,iyr2]*HSP_prob } } list(EC=EC,M=M) } Let’s looks at a few scenarios, including (1) sampling ages proportional to their approximate abundance in the population, and (2) sampling ages biased towards younger age classes. We’ll sample 20 individuals per year, as might occur in a balanced monitoring program. We’ll generate an expected number of comparisons and an expected count for each combination of birth years for half-sib comparisons (omitting same-cohort comparisons). Npy = rep(0,20) Npy[11:20] = 20 # sampling proportional to what we&#39;d expect with a constant survival probability of 0.92 age_wts_prop = 0.92^c(0:5) #since &quot;ages&quot; is 3:8, this needs to consist of 6 weights age_wts_young = c(6:1) #6 times more likely to sample &amp; genotype a 3 year old than an 8 year old Exp_data_prop = expected_data(Npy,ylo,yhi,ages,age_wts=age_wts_prop,N,phiA) Exp_data_young = expected_data(Npy,ylo,yhi,ages,age_wts=age_wts_young,N,phiA) One thing we can look at right away is the number of kin pairs for the two designs. For the design with proportional age sampling the number of HSPs is 57.6215; for the design focusing disproportionally on younger ages, we have 57.7955 HSPs. So, very close. 3.1.3 Calculating expected variance under different design scenarios Now let’s fit a “correct” model to these data - that is, one in which all assumptions are met. Note that we don’t actually need to fit a model to do design calculations, but TMB computes variance estimates and delta method approximations for functions of parameters, so we’ll sacrifice a tiny bit of time estimating parameters in order to make our lives easier. If we were going to consider a large number of designs (or to do formal optimization of a design using quadratic programming or something) we’d want to revisit this decision!! So, let’s compile a TMB model, fit a model to data, and look at estimated standard errors of various quantities. # compile TMB negative log pseudo-likeihood function library(TMB) compile(&quot;TMB/hsp_nll2.cpp&quot;) ## [1] 0 # format data and specify starting values for parameters format_data &lt;- function(M,EC){ Indices_gt0 = which(M&gt;0,arr.ind=TRUE) Data = list( n_HSP = EC[Indices_gt0], n_UP = M[Indices_gt0]-EC[Indices_gt0], born_year = Indices_gt0[,2], #for this HSP calc only need birth year of the *younger* animal age_diff = Indices_gt0[,2]-Indices_gt0[,1], present_year= 20 #terminal year for abundance estimate ) Data } Data_prop = format_data(M=Exp_data_prop$M,EC=Exp_data_prop$EC) Data_young = format_data(Exp_data_young$M,Exp_data_young$EC) Parms = list(&quot;N_init&quot; = 1000, &quot;lambda&quot;=1.0, phiA=0.94) dyn.load(dynlib(&quot;TMB/hsp_nll2&quot;)) obj &lt;- TMB::MakeADFun(data = Data_prop, parameters = Parms, DLL=&quot;hsp_nll2&quot;) Opt = nlminb(start=Parms, objective=obj$fn, gradient=obj$gr) ## outer mgc: 698.1 ## Warning in nlminb(start = Parms, objective = obj$fn, ## gradient = obj$gr): NA/NaN function evaluation ## outer mgc: 852.9 ## outer mgc: 376.3 ## outer mgc: 156.6 ## outer mgc: 89.72 ## outer mgc: 4.769 ## outer mgc: 2.079 ## outer mgc: 0.04171 ## outer mgc: 0.03488 ## outer mgc: 0.002186 ## outer mgc: 0.0001492 SD_report_prop=sdreport(obj) ## outer mgc: 0.0001492 ## outer mgc: 0.0005512 ## outer mgc: 0.0008495 ## outer mgc: 8.839 ## outer mgc: 8.957 ## outer mgc: 2.981 ## outer mgc: 2.966 ## outer mgc: 20000 obj &lt;- TMB::MakeADFun(data = Data_young, parameters = Parms, DLL=&quot;hsp_nll2&quot;) Opt = nlminb(start=Parms, objective=obj$fn, gradient=obj$gr) ## outer mgc: 732.1 ## Warning in nlminb(start = Parms, objective = obj$fn, ## gradient = obj$gr): NA/NaN function evaluation ## outer mgc: 762.4 ## outer mgc: 364 ## outer mgc: 140.8 ## outer mgc: 73.41 ## outer mgc: 12.91 ## outer mgc: 24.74 ## outer mgc: 28.85 ## outer mgc: 1.88 ## outer mgc: 0.23 ## outer mgc: 0.01301 ## outer mgc: 0.0005072 SD_report_young=sdreport(obj) ## outer mgc: 0.0005072 ## outer mgc: 0.0002272 ## outer mgc: 0.001242 ## outer mgc: 9.619 ## outer mgc: 9.753 ## outer mgc: 3.035 ## outer mgc: 3.02 ## outer mgc: 20000 # Okay, let’s see what TMB did for us. First let’s check that our estimates are truth - they should be very close since we’re using expected values. print(SD_report_prop$value) ## N_init lambda phiA N_last ## 1.000e+03 -1.864e-08 9.400e-01 1.000e+03 print(SD_report_young$value) ## N_init lambda phiA N_last ## 1.000e+03 -3.057e-08 9.400e-01 1.000e+03 Well that’s reassuring! How about standard errors? print(SD_report_prop$sd) ## [1] 616.47723 0.05528 0.05620 588.11186 print(SD_report_young$sd) ## [1] 667.27825 0.05704 0.05757 570.87206 So there is not much difference in estimator performance when sampling focuses on the youngest juvenile white sharks. Precision on \\(\\lambda\\) and survival is slightly better when we sample the full age range, while precision on terminal year abundance is slightly better when disproportionately focusing on very young white sharks. Although differences weren’t very large here, hopefully you can see how such an analysis might be used to get at “the most bang for your buck” when trying to design a CKMR monitoring program. 3.2 Bearded seal simulation study Next, we’ll return to the biology and approximate sampling scheme for bearded seals (presented as slides in earlier in this workshop). Although bearded seal samples have already been gathered, we could look at a number of different features of data collection that could be relevant for future monitoring. Some ideas include: What type of precision can we expect on adult abundance? How does our current approach of treating age as known tend to affect estimates? Are we likely to be over- or under-estimating abundance or survival with this strategy? Overstating precision? If this is a large issue, we might want to institute procedures for better quantifying aging error (such as collecting two teeth, having them both read, and fitting aging error models to the data) What is the effect of employing fecundity-at-age schedules that overrepresent young animals? For instance, even though age 6 male bearded seals are sexually mature, what if they are not as reproductively successful as older seals? What seal covariates (e.g., age, sex) should be prioritized for genotyping if we’re to maximize precision in abundance estimates going forward? Our time in this workshop is limited, however, so let’s just address the first of these for this lab (what kind of standard error can we expect?). 3.2.1 Setting up simulations: Population dynamics We’re going to use the R package CKMRpop to simulate some virtual bearded seal data, and fit an age structured CKMR model to these data. Given that it’s an age structured model, we’ll need to be careful that the simulation and estimation models are set up similarly - in particular we’ll want to make sure we get pre- and post-breeding census details correct. In fact, many published studies in the ecological literature have erred at this stage! (Kendall et al. (2019)). For bearded seals, pupping occurs from April-June on ice floes; harvests occur year round, but are primarily concentrated in the spring and summer. For this reason, we’ll use a “post-breeding census” where abundance is counted after reproduction has occurred. Pictorally, this looks like The part that people mess up most often is not incorporating survival into the fecundity term. We’ll include 40 ages (0-39) in our model, as the probability of a bearded seal surviving past that point is really small (I believe the oldest known age is in the 30s). Speaking of which, we’ll need to use some values of age-specific survival (\\(\\phi_a\\)) and female fecundity-at-age (\\(f_a\\)) to parameterize our simulation model. We’ll also need some information on male reproduction to determine who successfully mates or not. We’ll use data on male sexual maturity-at-age (call this \\(m_a\\)). We’ve developed a bearded seal survival schedule based on hierarchical meta-analysis of phocid natural mortality (Trukhanova et al. (2018)), and \\(f_a\\) and \\(m_a\\) schedules based on examination of gonadal inspections of harvested seals conducted by ADF&amp;G and reported in Russian literature. Here are some plots of these values. Let’s use these survival and fecundity values to parameterize a Leslie matrix model. Maturity = read.csv(&quot;./csv/Maturity.csv&quot;) Survival = read.csv(&quot;./csv/Survival_ests.csv&quot;) Reprod = read.csv(&quot;./csv/Reproduction_table.csv&quot;) Male_mat &lt;- rep(1,40) Fem_fec &lt;- rep(0.938,40) Male_mat[1:10]=c(0,Maturity$Bearded.male) Fem_fec[1:10]=c(0,Reprod$bearded) A = matrix(0,40,40) for(iage in 1:39){ A[iage+1,iage]=Survival[iage,&quot;bearded&quot;] #assume post-breeding census } #reproduction; nb: adults have to survive to next spring to reproduce # nb: Leslie matrices are &quot;female only&quot; and assume a 50/50 sex ratio at birth A[1,]=0.5*Fem_fec*Survival$bearded There’s a bit of a hiccup, however, since the population growth rate implied by this matrix (as determined by the dominant eigenvalue) is . Given that we’re going to need to perform simulation over 100 years (usually we have to go back two generations to get kinship relationships right), this would have the effect of inducing a 5400+0i percent increase in abundance over the simulation period!! This is clearly not desirable. It’s also an indication that there is probably something a bit off with our Leslie matrix, but it’s not clear where the bias might be. It could have to do with reproductively mature females not always producing a pup every year (\\(f_a\\) biased high), or with survival being overestimated. For the purposes of this simulation, though, we’ll lower survival by multiplying by a fixed constant until \\(\\lambda \\approx 1.0\\). Let’s write a quick function to figure out what this constant should be. leslie_obj &lt;- function(const,Survival,Fem_fec){ A = matrix(0,40,40) for(iage in 1:39){ A[iage+1,iage]=const*Survival[iage,&quot;bearded&quot;] #assume post-breeding census } A[1,]=0.5*Fem_fec*Survival$bearded*const lambda = eigen(A)$values[1] obj=(lambda-1.0)^2 obj } opt = nlminb(0.9,leslie_obj,Survival=Survival,Fem_fec=Fem_fec) print(opt$par,digits=4) ## [1] 0.9607 We’ve got it! Okay, we’ll multiply our current survival schedule by when we conduct our simulations - there will be some demographic stochasticity in any individual-based simulation, but given the large population size abundance should stay pretty much constant. 3.2.2 Setting up simulations: CKMRpop Our next step will be to use the R package CKMRpop to simulate population dynamics. If you were using the ‘CKMRpop’ package on your own, you would need to install it first like this: remotes::install_github(&quot;eriqande/CKMRpop&quot;) However, it has already been installed for the tws-ckmr-2022 RStudio project by the ‘renv’ package, so if you are working in the tws-ckmr-2022 you do not have to do the above. However, after installing it, you will have to download the compiled spip binary which is the underlying simulation engine. That is done with the install_spip() function from the ‘CKMRpop’ as shown below: library(CKMRpop) library(dplyr) install_spip(Dir = system.file(package = &quot;CKMRpop&quot;)) Now, let’s set up simulations with our bearded seal life history values. Note the quotation list structure is required by CKMRpop, and that the package is set up to work with a post-breeding census (good for us!). For definitions of the pars list, please use spip_help() or spip_help_full(). pars &lt;- list() pars$`max-age` &lt;- 40 #high survival so need to run for quite a while to kill everyone off pars$`fem-surv-probs` &lt;- pars$`male-surv-probs` &lt;- Survival$bearded*opt$par pars$`fem-prob-repro` &lt;- Fem_fec #probability of reproduction * pars$`repro-inhib` &lt;- 0 #this would allow us to specify e.g. that females couldn&#39;t have pups 2 years in a row (we don&#39;t have that issue!) pars$`male-prob-repro` &lt;- Male_mat pars$`fem-asrf` &lt;- rep(1,40) #if they reproduce, they will have at most 1 offspring pars$`male-asrp` &lt;- rep(1,40) #each reproductively mature male has same expected repro output pars$`offsp-dsn` &lt;- &quot;binary&quot; #override default neg. binomial dist. for litter size pars$`sex-ratio` &lt;- 0.5 Now we’ll set up some initial conditions and specify the length of the simulations. We’ll base the age structure of founders on stable age proportions computed using Leslie matrices, as calculated via the leslie_from_spip functionality in CKMRpop: pars$`number-of-years` &lt;- 120 # we need to specify initial cohort size instead of pop size; let&#39;s set it by assuming # a population size to be 400,000 and by using stable age proportions to figure out what # percentage will be pups N_init = 400000 Age_props = eigen(A)$vectors[,1]/sum(eigen(A)$vectors[,1]) cohort_size &lt;- as.numeric(round(N_init*Age_props[1])) # There&#39;s also some Leslie matrix functionality in spip we could use to set initial age strcutrue L &lt;- leslie_from_spip(pars, cohort_size) pars$`initial-males` &lt;- floor(L$stable_age_distro_fem) pars$`initial-females` &lt;- floor(L$stable_age_distro_male) pars$`cohort-size` &lt;- &quot;const 68737&quot; Note that when the cohort size argument is set to a constant, that the implied fecundity values in the Leslie matrix are rescaled so that the finite population growth rate, \\(\\lambda\\), is 1.0. This can be useful for simulation, but may make comparing fecundity values estimated by a CKMR model to those used to simulate the data difficult (although fecundity-at-age curves should be proportional). If this feature isn’t desired, CKMRpop can also be run with time-specific cohort sizes, or fishsim can be used for simulation. Next, we need to specify how the population is sampled. We’ll assume 20 years of sampling, which are preceded by 100 years of simulations time. This way we will likely capture the oldest possible grandparent-grandchild relationships (e.g., an individual who is age 35 at year 101 having a parent born in year 35 and a grandparent born in year 1). To be really careful we might have three generations time prior to sampling, but given that parents can’t reproduce until age 4 or 5, and that the chances of surviving to old age are very low, we should be fine. samp_start_year &lt;- 101 samp_stop_year &lt;- 120 pars$`discard-all` &lt;- 0 pars$`gtyp-ppn-fem-post` &lt;- paste( samp_start_year, &quot;-&quot;, samp_stop_year, &quot; &quot;, paste(rep(0.00025, pars$`max-age`), collapse = &quot; &quot;), sep = &quot;&quot; ) #basically we&#39;re saying that hunters aren&#39;t biased towards a particular age class, and that we&#39;re sampling 0.025% of the pop per year - about 100/year pars$`gtyp-ppn-male-post` &lt;- pars$`gtyp-ppn-fem-post` Okay, now that things are set up, we can run a CKMRpop simulation! This is super easy, although it takes a few minutes since we’re talking about simulating a populations size of 400,000 of 120 years and recording the geneaology along the way. I’m actually super impressed that this is so fast! I’ve put in an option for Rmarkdown to “cache” the results of this operation so that it doesn’t have to do it when this is compiled. So, just beware if you’re trying to run it on your own computer!! set.seed(2020) # inauspicious year as a seed for reproducibility of results spip_dir &lt;- run_spip(pars = pars) # run spip ## Running spip in directory /var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T//RtmpgOCSlV/file2c332338df03 ## Done running spip. Output file size is 1130.219259 Mb ## Processing output file with awk ## Done processing output into spip_pedigree.tsv, spip_prekill_census.tsv, and spip_samples.tsv slurped &lt;- slurp_spip(spip_dir, 2) # read the spip output into R ## Rows: 1627 Columns: 3 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;\\t&quot; ## chr (2): X1, X2 ## lgl (1): X3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ggplot_census_by_year_age_sex(slurped$census_postkill) The above plot shows the age and sex composition, as well as the total size of the population over time. For a design study the next thing we want to do is to start looking at sampled animals and number of kin pairs to make sure they line up with what we were hoping. nrow(slurped$samples) ## [1] 1627 crel &lt;- compile_related_pairs(slurped$samples) So, after 20 years of sampling (at an average of 81.35 samples per year), we end up with 1627 tissue samples. Assuming none are corrupted (a dubious assumption), they can be genetically analyzed and kin relationships can be determined. CKMRpop actually keeps track of a lot of relationships - more than are needed for CKMR analysis (at least in CKMR’s current state). Nevertheless, they can be useful because they can allow us to examine the performance of CKMR estimators when there are errors in kin determination (e.g., grandparent-grandchild or aunt-niece being mistaken for half-sibs). Here is a summary of true kin-pairs in our sample: relat_counts &lt;- count_and_plot_ancestry_matrices(crel) relat_counts$highly_summarised ## # A tibble: 5 × 3 ## dom_relat max_hit n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 FC 1 54 ## 2 A 1 30 ## 3 Si 1 12 ## 4 GP 1 4 ## 5 PO 1 4 Note that there are 54 half-cousins (FC), 30 half-niblings (aunt-niece, uncle-niece, etc.), 12 half-siblings, 4 grandparent-grandchild, and 4 parent-offspring pairs. Note that the “max_hit” column indicates that number of shared ancestors at the level of the dominant relationship; so a “2” in this column would indicate a full sibling, full aunt-niece, etc. Since the maximum value if 1 here, this indicates they are all half-sibling, half-aunt-niece, etc. Which is good, since presence of full niblings/thiatics would confound HSP inference. There are some additional graphical tools in CKMRpop to examine pairwise relationships (see the plot_conn_comps() function, or investigate the “plots” objects in the relat_counts object), but we won’t do that in this manual (free feel to do so yourself though!). What else may we deduce at this point? First, with only 4 POPs and 12 HSPs, we’re considerably below the “rule of thumb” of 50 kin pairs for reliable CKMR inference. These numbers may change slightly if we were to repeat the simulation, but the total number will be in the same ballpark. Second, we do get a substantial amount of GGPs, so we’ll need to accommodate these somehow, either by modeling or by restricting the time-of-death and age-ranges of HSP comparisons to make sure that GGPs are not included in HSP comparisons. We’re in the unique position of actually having conducted kin finding on \\(\\approx 1500\\) real bearded seal samples, so it is instructive to compare results of simulations to real data. When we analyzed real bearded seal data, we found 2 POPs, and depending on where we put the PLOD threshold, we arrived at 17-25 HSPs + GGPs. However, this number may have been inflated by a larger-than-expected number of PHSPs (arising, perhaps, from heterogeneity in male reproductive success). So simulations largely gave numbers very similar when we went and applied CKMR in practice, which is encouraging! 3.2.3 Formatting data for estimation Now that we’ve simulated some data, we need to put them into a form that we can analyze. For our bearded seal paper, I programmed a log pseudo-likelihood in TMB (see bearded_nll.cpp in the TMB directory of this project), and we will use that for estimation here. This version does not include the capacity for modeling GGPs, so let’s ignore that added complication for now (the final analysis does, but calculations are quite complex). For both POPs and HSPs, we need to summarize sufficient statistics for the number of comparisons and number of matches. A typical way to do this is by grouping according to relevant covariates. For POPs this includes birth dates of the two animals being compared, as well as the death date of the older animal (because the older animal can’t be a parent if it dies before the younger animal is conceived (for a poential male parent) or before the younger animal is weaned (for a potential female parent). We’ll thus summarize POP comparisons and matches using three-dimensional arrays, which facilitate easy looping when calculating pseudolikelihood. We’ll also do maternal and paternal POPs separately. For HSPs, death dates are unneeded; all we need is the birth dates for prospective HSPs. Looking at male and female fecundity-at-age vectors, it looks like we should probably limit comparisons to animals that are born within \\(approx 6 years\\) of each other to preclude the possibility that an apparent HSP is actually a GGP. We’ll lose two HSPs in the process, but such is life! # here, &#39;bi&#39;, &#39;di&#39;, and &#39;bj&#39; index birth of the older animal, death of the older animal, and birth of the younger animal # as far as dimensions, there are 20 years of data, but birth dates can be earlier (up one generation time) n_comp_PPO_bidibj=n_comp_MPO_bidibj=n_match_PPO_bidibj=n_match_MPO_bidibj=array(0,dim=c(60,20,60)) # HSPs are indexed by bi, bj - the birth times of the older and younger animal; note that # the number of comparisons does not depend on the sex of the unobserved parent, but matches are sex-specific n_comp_HS_bibj = n_match_PHS_bibj= n_match_MHS_bibj= matrix(0,60,60) # POP matches PO_only &lt;- crel %&gt;% filter(dom_relat == &quot;PO&quot;) for(i in 1:nrow(PO_only)){ FIRST = (PO_only$born_year_1[i] &lt; PO_only$born_year_2[i]) # case 1: parent is animal 1 if(FIRST){ p_by = PO_only$born_year_1[i]-60 p_sex = (PO_only$sex_1[i]==&quot;M&quot;)+1 #1 for female, 2 for males p_dy = PO_only$samp_years_list_1[[i]]-100 o_by = PO_only$born_year_2[i] - 60 #year 81 in simulation = year 1 for CKMR inference } else{ p_by = PO_only$born_year_2[i]-60 p_sex = (PO_only$sex_2[i]==&quot;M&quot;)+1 #1 for female, 2 for males p_dy = PO_only$samp_years_list_2[[i]]-100 o_by = PO_only$born_year_1[i] - 60 #year 81 in simulation = year 1 for CKMR inference } if(p_sex==1)n_match_MPO_bidibj[p_by,p_dy,o_by]=n_match_MPO_bidibj[p_by,p_dy,o_by]+1 else n_match_PPO_bidibj[p_by,p_dy,o_by]=n_match_PPO_bidibj[p_by,p_dy,o_by]+1 } # HSP matches HS_only &lt;- crel %&gt;% filter(dom_relat == &quot;Si&quot;) for(i in 1:nrow(HS_only)){ FIRST = (HS_only$born_year_1[i] &lt; HS_only$born_year_2[i]) lt7 = (abs(HS_only$born_year_1[i] - HS_only$born_year_2[i])&lt;7) # case 1: parent is animal 1 if(lt7){ if(FIRST){ by1 = HS_only$born_year_1[i] - 60 by2 = HS_only$born_year_2[i] - 60 #year 81 in simulation = year 1 for CKMR inference } else{ by1 = HS_only$born_year_2[i] - 60 by2 = HS_only$born_year_1[i] - 60 #year 81 in simulation = year 1 for CKMR inference } #to determine sex of parent, we need to access ancestry vectors and pull off an &quot;M&quot; for male or &quot;F&quot; for female parent_ID = HS_only$ancestors_1[i][[1]][unlist(HS_only$primary_shared_ancestors[i])[1]] sex = substr(parent_ID,1,1) if(sex==&quot;F&quot;)n_match_MHS_bibj[by1,by2]=n_match_MHS_bibj[by1,by2]+1 else n_match_PHS_bibj[by1,by2]=n_match_PHS_bibj[by1,by2]+1 } } #Comparisons n_indiv = nrow(slurped$samples) for(i1 in 1:(n_indiv-1)){ for(i2 in (i1+1):n_indiv){ born_diff = slurped$samples$born_year[i1] - slurped$samples$born_year[i2] if(abs(born_diff)&lt;7){ if(born_diff&lt;0){ #first animal is older p_by = slurped$samples$born_year[i1]-60 p_dy = slurped$samples$samp_years_list[i1][[1]]-100 p_sex = (slurped$samples$sex[i1]==&quot;M&quot;)+1 #1 for female, 2 for males o_by = slurped$samples$born_year[i2] - 60 #year 81 in simulation = year 1 for CKMR } else{ if(born_diff&gt;0){ #note no comparisons for animals born in same year p_by = slurped$samples$born_year[i2]-60 p_dy = slurped$samples$samp_years_list[i2][[1]]-100 p_sex = (slurped$samples$sex[i2]==&quot;M&quot;)+1 #1 for female, 2 for males o_by = slurped$samples$born_year[i1] - 60 #year 81 in simulation = year 1 for CKMR } } if(p_sex==1)n_comp_MPO_bidibj[p_by,p_dy,o_by] = n_comp_MPO_bidibj[p_by,p_dy,o_by] + 1 else n_comp_PPO_bidibj[p_by,p_dy,o_by] = n_comp_PPO_bidibj[p_by,p_dy,o_by] + 1 n_comp_HS_bibj[p_by,o_by]=n_comp_HS_bibj[p_by,o_by]+1 } } } 3.2.4 Fitting the CKMR model Okay, now we have our data formatted and it’s time to load up the jnll function from TMB, and pass data and initial parameter values to it to enable parameter estimation. Note that we’re going to pass fecundity schedules as known constants, and will handle survival by passing informative prior distributions into the estimation routine (it is a 3-parameter reduced-additive-Weibull model). Owing to low sample sizes, we’re also going to enforce a constraint of a constant population size. Here is some code to setup and run the model: Data=list(&quot;n_yrs&quot;=60,&quot;n_yrs_data&quot;=20,&quot;n_seals&quot;=nrow(slurped$samples),&quot;n_ages&quot;=40,&quot;Male_mat&quot;=Male_mat,&quot;Fem_fec&quot;=Fem_fec,&quot;A&quot;=A, &quot;n_match_PHS_bibj&quot;=n_match_PHS_bibj,&quot;n_match_MHS_bibj&quot;=n_match_MHS_bibj,&quot;n_comp_HS_bibj&quot;=n_comp_HS_bibj,&quot;n_match_MPO_bidibj&quot;=n_match_MPO_bidibj,&quot;n_comp_MPO_bidibj&quot;=n_comp_MPO_bidibj,&quot;n_match_PPO_bidibj&quot;=n_match_PPO_bidibj,&quot;n_comp_PPO_bidibj&quot;=n_comp_PPO_bidibj,mu_log_eta1=log(0.055),mu_log_eta2=log(2.8),mu_log_eta3=log(0.076),sd_log_eta1=0.07*abs(log(0.055)),sd_log_eta2=0.2*abs(log(2.8)),sd_log_eta3=abs(0.08*log(0.076)),lambda_expect=1.0) #SD log multipliers set to achieve approx CV of 0.2 on real scale Params = list(&quot;n0_log&quot;=log(20000),&quot;log_eta1&quot;=log(0.055),&quot;log_eta2&quot;=log(2.80),&quot;log_eta3&quot;=log(0.076)) #intial param values Map = list() #specify fixed parameter values #Random= c(&quot;log_eta1&quot;,&quot;log_eta2&quot;,&quot;log_eta3&quot;) Random=NULL library(TMB) getwd() ## [1] &quot;/Users/runner/work/tws-ckmr-2022/tws-ckmr-2022&quot; TmbFile = &quot;./TMB/bearded_nll.cpp&quot; compile(TmbFile ) ## [1] 0 TmbExec=&quot;./TMB/bearded_nll&quot; dyn.load(dynlib(TmbExec)) Obj &lt;- MakeADFun(data=Data, parameters=Params, random=Random, map=Map, hessian=FALSE, DLL=&quot;bearded_nll&quot;) Obj$fn( Obj$par ) ## [1] 2176425 Report = Obj$report() init_report = Report #Minimize negative log likelihood and time it Start_time = Sys.time() Opt = nlminb(start=Params, objective=Obj$fn, gradient=Obj$gr) ## outer mgc: 20936170 ## outer mgc: 8511535 ## outer mgc: 1708844 ## outer mgc: 1296351 ## outer mgc: 846387 ## outer mgc: 38796 ## outer mgc: 17648 ## outer mgc: 354.1 ## outer mgc: 2343 ## outer mgc: 180.7 ## outer mgc: 1431 ## outer mgc: 2875 ## outer mgc: 1552 ## outer mgc: 1150 ## outer mgc: 45.01 ## outer mgc: 180.9 ## outer mgc: 488.8 ## outer mgc: 123.3 ## outer mgc: 12.81 ## outer mgc: 4.869 ## outer mgc: 0.3811 ## outer mgc: 3.208 ## outer mgc: 8.661 ## outer mgc: 5.924 ## outer mgc: 45.47 ## outer mgc: 7.402 ## outer mgc: 1.951 ## outer mgc: 3.413 ## outer mgc: 3.515 ## outer mgc: 2.067 ## outer mgc: 0.2946 ## outer mgc: 0.02022 End_time = Sys.time() Report=Obj$report() SD_report=sdreport(Obj) ## outer mgc: 0.02022 ## outer mgc: 0.06032 ## outer mgc: 0.0199 ## outer mgc: 63308 ## outer mgc: 63004 ## outer mgc: 1107 ## outer mgc: 1110 ## outer mgc: 50079 ## outer mgc: 50038 ## outer mgc: 1406518 N_est_TMB = SD_report$value[which(names(SD_report$value)==&quot;N&quot;)] #check for convergence Opt$message ## [1] &quot;relative convergence (4)&quot; Okay, so it looks like our model converged, so it’s time to check on results. It looks like we’re estimating abundance as 2.710410^{5} with a standard error of 7.25610^{4} - so the CV is 0.2677. That’s not bad! But, remember we’ve employed a large number of assumptions (constant abundance, etc.) to get an estimate with such low sample sizes. Our estimate (270,000) is lower than true abundance (400,000) but if we constructed a confidence interval it would include the true value, so it’s not something to be worried about. If we wanted to quantify estimator performance in a more rigorous way, we’d want to conduct a large number of simulations and look at things like bias, confidence interval coverage, etc. How about survival? Recall that CKMR can inform estimation of adult survival, which is why we used a prior on the three parameter survival-at-age function. Presumably the prior specifies all the information on juvenile and subadult survival, but the HSP data provide additional information on adult survival. There’s actually another hidden piece of information, and that is the constraint that population size is constant (basically a \\(\\lambda=1.0\\) constraint). So when we compare the prior survival model to the posterior survival model we see quite a shift - but a lot of that has to do with \\(\\lambda=1.0\\). Plot_df = data.frame(&quot;Type&quot;=rep(c(&quot;Prior&quot;,&quot;CKMR&quot;),each=40), &quot;Value&quot;=c(Survival$bearded,Report$S_a), &quot;Age&quot;= rep(c(0:39),2)) library(ggplot2) ggplot(Plot_df)+geom_line(aes(x=Age,y=Value,colour=Type),size=1.1)+theme(text=element_text(size=16)) 3.2.5 Thought experiment Info! What else about the bearded seal example would make sense to examine using simulation?? References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
