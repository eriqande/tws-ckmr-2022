[["index.html", "Close-kin mark-recapture: theory and practice — Spokane, Washington, USA Preface 0.1 Workshop schedule 0.2 Resources 0.3 Setting up your computer", " Close-kin mark-recapture: theory and practice — Spokane, Washington, USA Paul B. Conn and Eric C. Anderson 2022-10-22 Preface This is the website/book associated with the Close-kin mark-recapture: theory and practice workshop to be held at The Wildlife Society meetings Nov 6, 2022 in Spokane, Washington. 0.1 Workshop schedule An approximate schedule for the workshop is as follows: 8:00 - 8:45 Close-kin mark-recapture: An overview (P. Conn) 8:45 - 9:30 An introduction to genetic data and inheritance (E. Anderson) 9:30 - 9:45 Break 9:45 - 10:30 Statistical inference for CKMR estimation (P. Conn) 10:30 - 11:15 Kin finding (E. Anderson) 11:15 - 12:00 Designing a CKMR experiment 12:00 - 1:00 Lunch 1:00 - 5:00 R/TMB labs (full day participants only) Workshop attendees had the option of registering for half-day or full-day sessions. The first half will consist almost entirely of lectures, with no previous programming experience needed. The second half will involve labs and exercises conducted in R and/or Template Model Builder (at the user’s discretion). Full day participants should use the following set of instructions to set up their computers prior to the workshop. 0.2 Resources This “book” as well as slides for the workshop are located on github at https://github.com/eriqande/tws-ckmr-2022 There is also additional material and examples that may be of interest at https://closekin.github.io/ 0.3 Setting up your computer This course covers topics in close-kin mark recapture, and the second half of the course relies heavily on the R programming language. In order to follow along with the code and be successful in running all of the examples, it will be helpful (if not imperative) for full-day workshop participants to have recent versions of R and RStudio, and updated versions of a number of packages. The following is a description of the software needed to engage in the course. This setup was tested on a Mac with an Intel chip running BigSur OSX 11, as well as on a PC running Windows 10. We suspect it will also work on most other Mac or Windows operating systems. 0.3.1 Step 2. Install a number of R packages that are relatively easy to install Our work will require a number of packages that can be found in binary form on CRAN. As such, installing them is typically not to arduous. This year, we are exploring use of the ‘renv’ package to help with installing necessary packages. 0.3.2 Step 3. Make sure you have git and an account on GitHub In a two-day workshop, we don’t have time to go deeply, if much at all, into the many uses of the version control software, git, and the cloud-based code management system GitHub, that is built upon git. But, if you are interested in version control for your analyses, and you are interested in using GitHub to share and present the results of your research, then you really will want to become proficient with both git and GitHub. Fortunately, there is an outstanding, free book on the web that goes into great detail about how to use git and GitHub with R and RStudio. It is available at https://happygitwithr.com/, and it is well worth a read, and particularly following the steps in: Chapter 4. Register a GitHub account. Chapter 6. Install Git (Note: Mac users , if xcode-select --install ran successfully, then git will have been installed). Chapter 7. Introduce yourself to git If you want to use GitHub, you will also have to establish an SSH public/private key pair to authenticate your computer to GitHub. That is described in: Chapter 10: Set up keys for SSH "],["ckmr-thought-experiments-and-labs.html", "Session 1 CKMR thought experiments and labs 1.1 Afternoon schedule 1.2 Thought experiments", " Session 1 CKMR thought experiments and labs In the second half of this course, we’re going to switch gears and delve into some code. But first, we’re going to conduct a few thought experiments to try to get you to think critically about some of the CKMR concepts we’ve covered so far. We’ll also be switching from slides to this “book” - hopefully this will be a useful resource for those of you who want to look back at course content. 1.1 Afternoon schedule An approximate schedule for the afternoon is 1:00 - 1:30 Thought experiments 1:30 - 2:30 Kin finding lab 2:30 - 2:45 Break 2:45 - 3:30 White shark estimation example 3:30 - 4:30 CKMR Design 4:30 - 5:00 Open! Feel free to play with code or ask us questions 1.2 Thought experiments We think it would be useful for participants to break into small groups for this one. So let’s do that! 1.2.1 Thought experiment 1 "],["simulation-and-inference-play-with-hillary-et-al.s-white-shark-ckmr-example.html", "Session 2 Simulation and Inference Play with Hillary et al.’s White Shark CKMR example 2.1 Simulating from their inferential model 2.2 An R function to compute the negative log-likelihood 2.3 Evaluate the likelihood with TMB 2.4 Visualize those log likelihood values 2.5 OMG! Let’s investigate the tmbstan package!", " Session 2 Simulation and Inference Play with Hillary et al.’s White Shark CKMR example Here we explore the models used in Hillary et al. (2018) to estimate abundance of white sharks in Australia. Looking through the paper, I didn’t find any direct mention of a public repository of data that was used in the paper, nor any repository of code for their analyses. So, we are just going to simulate some data here from their model (first just from their inference model, and maybe later we will do so with a simple individual-based-model) so that we have something to work with. One nice thing about simulating from their inference model is that doing so offers a good way to gain a better understanding their inference model. 2.1 Simulating from their inferential model They start with a demographic model of exponential growth: \\[ N_t^A = N_\\mathrm{init}e^{\\lambda t} \\] where \\(N_t^A\\) is the number of adults at time \\(t\\) and \\(N_\\mathrm{init}\\) is the initial number of adults (at time \\(t = 0\\)). \\(\\lambda\\) is an exponential growth rate. Here, we define a function to calculate \\(N_t^A\\): library(tidyverse) library(microbenchmark) library(TMB) #&#39; calculate the number of adults at time t, given exponential growth or decline #&#39; @param N0 number of adults at time 0 #&#39; @param t time #&#39; @param lambda exponential growth rate parameter N_adults &lt;- function(N0, t, lambda) { N0 * exp(lambda * t) } Let’s see what that would look like over 60 years, starting from a few different \\(N_\\mathrm{init}\\) values and growing/shrinking at a range of values. A_traj &lt;- expand_grid( Ninit = c(600, 800, 1000, 1200, 1500, 2000), lambda = seq(-0.03, 0.03, by = 0.01) )%&gt;% mutate( num_adults = map2(Ninit, lambda, function(x, y) N_adults(x, 0:60, y)), ) %&gt;% unnest(num_adults) %&gt;% group_by(Ninit, lambda) %&gt;% mutate(year = 0:60) ggplot(A_traj, aes(x = year, y = num_adults, colour = factor(lambda))) + geom_line() + facet_wrap(~ Ninit) Now we are going to write a function to simulate sampled pairs from such a population, and then we will simulate some of them to be half-siblings according to the probabilities in their inferential model. #&#39; simulate sampling n sharks a year from year Slo to Shi and return all the sample pairs #&#39; @param npy number of sharks to sample each year (num per year) #&#39; @param Slo year to start sampling #&#39; @param Shi final year in which to sample #&#39; @param ages a vector of the ages at which individuals are sampled #&#39; @param age_wts vector of expected proportion of each age in the sample #&#39; @param N0 initial population size #&#39; @param lambda population growth rate #&#39; @param Thi final time to simulate an abundance (must be &gt;= than Shi) #&#39; @param phiA Adult annual survival probability #&#39; @return This returns a list of three components: `pairs`: a tibble of all the pairs, #&#39; `pair_counts`: a tibble of counts of different pair types of different age differences, #&#39; and `samples`: a simple tibble of just the samples, and `N`: a tibble of the #&#39; number of adults each year. simulate_pairs &lt;- function( npy = 10, Slo = 20, Shi = 40, ages = 3:8, age_wts = rep(1, length(ages)), N0 = 800, lambda = 0.01, Thi = 60, phiA = 0.94 ) { A_traj &lt;- tibble( year = 0:Thi ) %&gt;% mutate( num_adults = N_adults(N0, year, lambda) ) samples &lt;- tibble( samp_year = rep(Slo:Shi, each=npy) ) %&gt;% mutate( age = sample(ages, size = n(), replace = TRUE, prob = age_wts), born_year = samp_year - age ) %&gt;% left_join(A_traj, by = c(&quot;born_year&quot; = &quot;year&quot;)) n &lt;- nrow(samples) s1 &lt;- samples[rep(1:n, each = n),] s2 &lt;- samples[rep(1:n, times = n),] names(s2) &lt;- paste0(names(s2), &quot;.old&quot;) pairs &lt;- bind_cols(s1, s2) %&gt;% filter(born_year.old &lt; born_year) %&gt;% mutate( age_diff = born_year - born_year.old, HSP_prob = (4/num_adults) * (phiA ^ (born_year - born_year.old)), isHSP = rbernoulli(n(), p = HSP_prob) ) pair_counts &lt;- pairs %&gt;% count(born_year, age_diff, HSP_prob, isHSP) %&gt;% pivot_wider(names_from = isHSP, values_from = n, values_fill = 0L) %&gt;% rename(n_UP = `FALSE`, n_HSP = `TRUE`) list( pairs = pairs, pair_counts = pair_counts, pair_data = pair_counts %&gt;% select(-HSP_prob), samples = samples, N = A_traj ) } Now that we have done that, let’s simulate some pairs. We will assume that 20 sharks were sampled per year—that is more than Hillary et al. (2018) had, I think, but it makes the likelihood surface a little cleaner, so, since we can simulate whatever we want, we will simulate an abundance of data: set.seed(5) sim_vals &lt;- simulate_pairs(npy = 20, age_wts = 8:3) At this juncture, it is worth stopping for a moment and looking at two of the elements of the list sim_vals. The pair_counts show the number of half-sibling pairs of different types, and also shows the half-sibling pair probability that was used to simulate them: sim_vals$pair_counts ## # A tibble: 325 × 5 ## born_year age_diff HSP_prob n_UP n_HSP ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 13 1 0.00413 10 0 ## 2 14 1 0.00409 35 0 ## 3 14 2 0.00384 14 0 ## 4 15 1 0.00405 84 0 ## 5 15 2 0.00380 60 0 ## 6 15 3 0.00357 24 0 ## 7 16 1 0.00401 216 0 ## 8 16 2 0.00376 124 2 ## 9 16 3 0.00354 90 0 ## 10 16 4 0.00333 36 0 ## # … with 315 more rows ## # ℹ Use `print(n = ...)` to see more rows In this tibble, n_HSP is the number of half-sibling pairs found in which the younger member was born in born_year and the older member was born age_diff years before. As you can see, there are a lot of categories for which no HSPs are found. That is to be expected—they are pretty rare, and, of course, you expect to see fewer of them if the population is large…that is how CKMR works… The HSP_prob column gives the probability of seeing an HSP of a certain category given the values of the parameters that were used to simulate the data. Typically if you had some observed data, you wouldn’t compute the HSP_prob for parameter values you didn’t know. So, if you had observed data it would look like: sim_vals$pair_data ## # A tibble: 325 × 4 ## born_year age_diff n_UP n_HSP ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 13 1 10 0 ## 2 14 1 35 0 ## 3 14 2 14 0 ## 4 15 1 84 0 ## 5 15 2 60 0 ## 6 15 3 24 0 ## 7 16 1 216 0 ## 8 16 2 124 2 ## 9 16 3 90 0 ## 10 16 4 36 0 ## # … with 315 more rows ## # ℹ Use `print(n = ...)` to see more rows And the goal from those data would be to estimate the parameters that might have produced these data. It is, however, worthwhile to plot the half-sibling pair probabilities, just to look at those. sim_vals$pairs %&gt;% group_by(born_year, age_diff) %&gt;% summarise(HSP_prob = mean(HSP_prob)) %&gt;% ggplot(aes(x = born_year, y = HSP_prob, fill = factor(age_diff))) + geom_point(shape = 21) + xlab(&quot;Year the younger member of pair was born&quot;) + ylab(&quot;Half-sibling probability&quot;) ## `summarise()` has grouped output by &#39;born_year&#39;. You ## can override using the `.groups` argument. 2.2 An R function to compute the negative log-likelihood To estimate that parameters that gave rise to the simulated data, we can start with an R function to compute the negative log-likelihood—in other words, the probability of our observed data given values of the parameters, which are the initial population size \\(N_\\mathrm{init}\\), the population growth rate \\(\\lambda\\), and the probability that an adult survives for a year, \\(\\phi_A\\). The following R function computes that CKMR pseudolikelihood: #&#39; Return the negative log likelihood of the parameter values given the data #&#39; @param pars a vector (Ninit, lambda, phiA) #&#39; @param X a tibble like the pair_counts component of the output list from #&#39; `simulate_pairs()`. hsp_nll &lt;- function(pars, X) { N0 &lt;- pars[1] L &lt;- pars[2] P &lt;- pars[3] LL &lt;- X %&gt;% mutate( N = N0 * exp(L * born_year), hspp = (4 / N) * P ^ age_diff, logl = log(hspp) * n_HSP + log(1 - hspp) * n_UP ) -sum(LL$logl) } Let’s first compute the negative log-likelihood for the values used in the simulation: hsp_nll( pars = c(800, 0.01, 0.94), X = sim_vals$pair_data ) ## [1] 1508 Since this is a negative log likelihood, a smaller number is better. We can see that if we choose parameter values that are far from the true ones we get a bigger (less good) value of the negative log likelihood. hsp_nll( pars = c(1800, 0.05, 0.94), X = sim_vals$pair_data ) ## [1] 1752 It is a fun exercise to visualize this log-likelihood surface. But we are not going to do that with this hsp_nll function, because it is quite slow. Observe how many milliseconds it takes to evaluate the likelihood using the hsp_nll() function: mb &lt;- microbenchmark::microbenchmark( R_version = hsp_nll( pars = c(1800, 0.05, 0.94), X = sim_vals$pair_data ), times = 100 ) # on average, typically more than 2 milliseconds: mb ## Unit: milliseconds ## expr min lq mean median uq max neval ## R_version 2.716 2.898 3.019 2.991 3.09 3.795 100 In general, straight-up R is a little underpowered for computing these sorts of likelihoods, and there are some serious advantages to calculating them in compiled code using the package TMB. We will do that next. 2.3 Evaluate the likelihood with TMB You need the following code in a file TMB/hsp_nll.cpp, relative to the current working directory. (That file is already in this repository). // simple half-sibling CKMR with exponential pop growth and known ages #include &lt;TMB.hpp&gt; template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { DATA_VECTOR(n_UP); DATA_VECTOR(n_HSP); DATA_VECTOR(born_year); DATA_VECTOR(age_diff); PARAMETER(N_init); PARAMETER(lambda); PARAMETER(phiA); ADREPORT(N_init); ADREPORT(lambda); ADREPORT(phiA); Type N; // for storing the pop size at time of younger kids birth Type hsp; // for storing the half-sibling pair prob Type nll = 0; for(int i=0;i&lt;n_UP.size(); i++) { N = N_init * exp(lambda * born_year[i]); hsp = (4 / N) * pow(phiA, age_diff[i]); nll -= log(hsp) * n_HSP[i] + log(1 - hsp) * n_UP[i]; } return nll; } That file first must be compiled. compile(&quot;TMB/hsp_nll.cpp&quot;) ## [1] 0 After that we will make some data and parameter lists: # then get our data in a list of vectors data &lt;- sim_vals$pair_data %&gt;% select(n_UP, n_HSP, born_year, age_diff) %&gt;% as.list() # and then our starting parameters parameters &lt;- list(N_init = 1000, lambda = 0.001, phiA = 0.9) Now, to make an AD function with TMB, we need to link to the library created when we compiled the file above. For unknown reasons,on a Mac, TMB seems unable to deal with having the compiled object files in a subdirectory, so, in the following code chunk we test to see if we are on a Mac, and if we are we momentarily switch to the TMB directory. Otherwise we just run the functions the way we should be able to. if(Sys.info()[&quot;sysname&quot;] == &quot;Darwin&quot;) { setwd(&quot;TMB&quot;) dyn.load(dynlib(&#39;hsp_nll&#39;)) obj &lt;- TMB::MakeADFun(data = data, parameters = parameters, DLL=&quot;hsp_nll&quot;) setwd(&quot;..&quot;) } else { dyn.load(dynlib(&quot;TMB/hsp_nll&quot;)) obj &lt;- TMB::MakeADFun(data = data, parameters = parameters, DLL=&quot;hsp_nll&quot;) } Now, we might want to see how long it takes for this compiled version of the HSP negative log likelihood to be computed: mb2 &lt;- microbenchmark( TMB_version = obj$fn(x = parameters), times = 1000 ) # this is in microseconds, so it is effectively 100 times faster # to evalaute this using TMB. mb2 ## Unit: microseconds ## expr min lq mean median uq max neval ## TMB_version 32.7 33.49 65 33.75 34.05 7564 1000 2.4 Visualize those log likelihood values Now that we have a fast way to evaluate those, using TMB, let’s evaluate a lot of log likelihoods so we can make some contour plots. We will evaluate the log likelihood over a range of values of the three parameters: LL_tib &lt;- expand_grid( N_init = seq(200, 2000, by = 20), lambda = seq(-0.04, 0.04, by = 0.01), phiA = seq(0.85, 1, by = 0.01) ) %&gt;% mutate(parameters = pmap( .l = list(a = N_init, b = lambda, c= phiA), .f = function(a,b,c) list(N_init = a, lambda = b, phiA = c) )) %&gt;% mutate( nll = map_dbl( .x = parameters, .f = function(y) obj$fn(x = y) ) ) With all those results, we can make a contour plot, faceted over lambda values to visualize: LL_tib %&gt;% group_by(lambda) %&gt;% mutate(nll = ifelse(nll &gt; min(nll) + 20, NA, nll)) %&gt;% ungroup() %&gt;% ggplot(aes(x = N_init, y = phiA, z = nll)) + geom_contour(binwidth = 1, colour = &quot;gray&quot;) + theme(legend.position = &quot;none&quot;) + facet_wrap(~lambda) + theme_bw() + geom_vline(xintercept = 800, colour = &quot;blue&quot;) + geom_hline(yintercept = 0.94, colour = &quot;red&quot;) ## Warning: Removed 9477 rows containing non-finite values ## (stat_contour). I am sure there is a cleaner way of dropping all the contour lines smaller than a certain value, but I am not going to bother with it here. The blue and red lines show where the actual “true” simulated values of \\(N_\\mathrm{init}\\) and \\(\\phi_A\\) are, respectively. Recall that the true value of \\(\\lambda\\) is 0.01. And see on that facet that the true values of \\(N_\\mathrm{init}\\) and \\(\\phi_A\\) are near the top of the likelihood (or the bottom of the negative log likelihood, if you prefer to think of it that way). We see that with this much data, and whilst assuming the correct growth rate, the abundance and the adult survival are estimated quite accurately. The question remains, however, of how much information there is about \\(\\lambda\\). Can we estimate that well? 2.5 OMG! Let’s investigate the tmbstan package! One very nice feature of using TMB is that we can plug our TMB object directly into Stan for doing Bayesian inference via No-U-turn MCMC sampling. We do this with the ‘tmbstan’ package. Awesome! Note that we sort of need to impose some bounds on the parameters. Otherwise, as one might expect seeing the contour plots of the likelihood surfaces, we can get estimated survival parameters exceeding 1.0, which just doesn’t make sense. This is one particularly nice feature of using the TMB object in a Bayesian context—it seems easier to impose these sorts of parameter bounds, than when using the TMB object to maximize the likelihood. For some reason that I don’t full understand, we have to remake the TMB AD function before we run the following. Something about having used obj$fn to compute the values of the Neg Log Likelihood, in order to make our pretty contour plots, has gummed things up. But no worries, we just define it again: setwd(&quot;TMB&quot;) dyn.load(dynlib(&#39;hsp_nll&#39;)) obj &lt;- MakeADFun(data = data, parameters = parameters, DLL=&quot;hsp_nll&quot;) setwd(&quot;..&quot;) Now we can hand that to tmbstan and do a short MCMC run: library(tmbstan) ## Loading required package: rstan ## Loading required package: StanHeaders ## rstan (Version 2.21.5, GitRev: 2e1f913d3ca3) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) ## ## Attaching package: &#39;rstan&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract set.seed(10) fit &lt;- tmbstan( obj, chains=1, lower = c(N_init = 20, lambda = -0.5, phiA = 0.2), upper = c(N_init = Inf, lambda = 0.5, phiA = 1.0), iter = 7500 ) ## ## SAMPLING FOR MODEL &#39;tmb_generic&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000106 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.06 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 7500 [ 0%] (Warmup) ## Chain 1: Iteration: 750 / 7500 [ 10%] (Warmup) ## Chain 1: Iteration: 1500 / 7500 [ 20%] (Warmup) ## Chain 1: Iteration: 2250 / 7500 [ 30%] (Warmup) ## Chain 1: Iteration: 3000 / 7500 [ 40%] (Warmup) ## Chain 1: Iteration: 3750 / 7500 [ 50%] (Warmup) ## Chain 1: Iteration: 3751 / 7500 [ 50%] (Sampling) ## Chain 1: Iteration: 4500 / 7500 [ 60%] (Sampling) ## Chain 1: Iteration: 5250 / 7500 [ 70%] (Sampling) ## Chain 1: Iteration: 6000 / 7500 [ 80%] (Sampling) ## Chain 1: Iteration: 6750 / 7500 [ 90%] (Sampling) ## Chain 1: Iteration: 7500 / 7500 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 3.89677 seconds (Warm-up) ## Chain 1: 4.26565 seconds (Sampling) ## Chain 1: 8.16243 seconds (Total) ## Chain 1: # and here is a summary of the result: fit ## Inference for Stan model: hsp_nll. ## 1 chains, each with iter=7500; warmup=3750; thin=1; ## post-warmup draws per chain=3750, total post-warmup draws=3750. ## ## mean se_mean sd 2.5% 25% ## N_init 915.51 10.08 351.90 416.03 668.15 ## lambda 0.00 0.00 0.01 -0.02 -0.01 ## phiA 0.93 0.00 0.02 0.89 0.91 ## lp__ -1505.94 0.04 1.29 -1509.39 -1506.50 ## 50% 75% 97.5% n_eff Rhat ## N_init 860.22 1079.69 1780.87 1218 1 ## lambda 0.00 0.01 0.03 1209 1 ## phiA 0.92 0.94 0.96 1324 1 ## lp__ -1505.61 -1505.00 -1504.50 954 1 ## ## Samples were drawn using NUTS(diag_e) at Fri Oct 21 19:00:41 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Anyone who has spent months trying to “roll their own” MCMC sampler will recognize that this was uncharacteristically painless. That is part of the joy of being able to calculate gradients (with the auto-differentiation features of TMB) to use those in Hamiltonian Monte Carlo. Also, the maintainers of Stan, TMB, and tmbstan have done an amazing job! If you want a really slick visual representation of the results from the MCMC, you can do: library(shinystan) launch_shinystan(fit) We don’t do that in this Rmarkown document, because you need to be in a an interactive session for it to work, but, just know that it pops up a shiny window like this: Also, if we wanted to start our chain from different starting values and run it in parallel on different cores, we can do that like this, though it seems the interface to this has changed a little. Try ?tmbstan for details. cores &lt;- parallel::detectCores()-1 options(mc.cores = cores) set.seed(1) init.list &lt;- lapply(1:cores, function(x) { c( N_init = runif(1, min = 2000, max = 20000), lambda = runif(1, -0.04, 0.04), phiA = runif(1, 0.75, 0.999) )} ) fit2 &lt;- tmbstan( obj, chains=cores, open_progress=FALSE, init=init.fn, lower = c(N_init = 20, lambda = -0.5, phiA = 0.2), upper = c(N_init = Inf, lambda = 0.5, phiA = 1.0), iter = 6000, warmup = 2000 ) That is remarkably easy and slick. I’m impressed. References "],["design-of-ckmr-experiments.html", "Session 3 Design of CKMR Experiments 3.1 White shark design example 3.2 Fisher information ideas 3.3 Calculating expected data under different design scenarios 3.4 Calculating expected variance under different design scenarios", " Session 3 Design of CKMR Experiments In this lab, we’ll look at various tools to help design CKMR experiments. We’re relatively liberal with what we call a “design” tool. It could be optimizing sample allocation to achieve maximum precision for a given parameter (e.g., which sexes and ages to target), or it could simply be examining the effect of different types of assumption violations on estimator performance. After all, if a particular type of model is likely to lead to highly biased parameter estimates, it is probably best to adapt model structure, or to leave out certain types of kin comparisons that are likely to violate assumptions. We’ll consider two specific examples: (1) optimizing sample allocation in Hillary et al. (2018) white shark example, and (2) conducting individual-based simulation to investigate impacts of assumption violations in CKMR models for “weirded seals”. As a reminder, weirded seals are our pet name for bearded seals in and around Alaska, but with some of the data changed to protect their integrity prior to the bearded seal study actually being published. 3.1 White shark design example In a previous lab, we looked at Hillary et al. (2018) model using cross-cohort comparisons of half siblings caught as juveniles. We might look at a few things with this example, such as how we would partition samples across years and ages if we had the ability to do so. Presumably, the optimal allocation would depend on the parameter we were interested in (e.g., population growth (\\(\\lambda\\)), adult survival (\\(S\\)) or terminal year abundance (call this \\(N_T\\)). Both \\(\\lambda\\) and \\(S\\) are parameters of the CKMR model, but \\(N_T\\) is a derived parameter (that is, a function of parameters) and can be computed as \\[\\begin{equation*} N_T = N_0 * \\exp(\\lambda T) \\end{equation*}\\] where \\(T\\) is the number of years in the study. 3.2 Fisher information ideas We’ll be using Fisher information ideas to compare anticipated precision associated with various white shark sampling designs. As articulated in the slides on design for this workshop, this involves a number of steps: Allocate potential sample sizes to particular years, ages, sexes, etc. (basically the different covariates one is modeling). These can be expectations (fractions)! Compute sufficient statistics (expected # of comparisons and number of matches for each covariate combination) Treating these as data, calculate second derivatives of the negative log-pseudo-likelihood at the true parameter values Calculate the expected variance-covariance matrix of parameters in the usual way (inverse of the numeric Fisher information matrix) Calculate expected variance of any functions of parameters using the delta method Compare precision (e.g. CV) associated with different designs!! Fortunately, we already have the infrastructure set up to help us the negative log-pseudo-likelihood (both in R and TMB). We could use numerical second derivatives (e.g., using the \\(numDeriv\\) package in R) and not even have to deal with minimizing the NLPL, but Template Model Builder is well set up to produce numerical variance-covariance matrices (including functions of parameters), so why don’t we just follow the path of least resistance and fit our model using TMB. We’ll still need to formulate some alternative designs and what the expected data would look like under each. 3.3 Calculating expected data under different design scenarios Let’s set up “true” population dynamics to consist of a stable population with 1000 adults which we assume to be constant over a 20 year period. We’ll set the assume a constant adult survival probability of 0.94, and that sampling occurs over the last 10 years of the time interval. We’ll assume that sampling targets juveniles (ages 3-8) only, so the most we’ll have to go back in time is 8 years (which is why the model needs to go back further in time than just the years that are sampled!!). Ninit = 1000 lambda = 0.0 n_yrs = 20 N = matrix(Ninit,20) phiA = 0.94 ylo=11 yhi=20 ages = c(3:8) #only 3 - 8 yrs olds sampled #&#39; Function to calculate # of expected pairs given true adult abundance, adult #&#39; survival, and # of juvenile white sharks sampled per year (males and female parents modeled together!) #&#39; @param Npy vector of number of sharks to sample each year (num per year) #&#39; @param ages a vector of the ages at which individuals are sampled #&#39; @param age_wts vector of expected proportion of each age in the sample #&#39; @param N Vector of number of adults per year #&#39; @param phiA Adult annual survival probability #&#39; @return This returns two upper triangular matrices `M` and `EC`, both of which have dimension #&#39; (n_yrs x #&#39; n_yrs). The M matrix holds the number of comparisons where the row indexes the #&#39; birth year of the older #&#39; half-sib, and the column gives the birth year of the younger #&#39; half sib. The EC matrix is organized the same way, but holds the expected half-sib count #&#39; (i.e. # of matches) expected_data &lt;- function(Npy,ylo,yhi,ages,age_wts,N,phiA){ age_wts = age_wts/sum(age_wts) #normalize if not n_ages = max(ages)+1 #nb this is just age 0 to the max of sampled age age_prob = rep(0,n_ages) age_prob[ages+1]=age_wts/sum(age_wts) n_yrs = length(Npy) M = EC = matrix(0,n_yrs,n_yrs) # expected number sampled by year and age N_samp = matrix(0,n_yrs,max(ages)+1) #this holds expected number sampled by year and age (w/ age 0) for(iyr in 1:n_yrs){ N_samp[iyr,]=Npy[iyr]*age_prob } # convert to number sampled by birth year N_samp_by = rep(0,n_yrs) for(iyr in 11:n_yrs){ #this would need to be changed to make general - i.e., sampling occurred &lt; year 10 for(iage in 1:n_ages){ N_samp_by[iyr-iage+1]=N_samp_by[iyr-iage+1]+N_samp[iyr,iage] } } #Number of comparisons, probability of matches, expected number of matches for(iyr in 1:(n_yrs-1)){ for(iyr2 in (iyr+1):n_yrs){ M[iyr,iyr2]=N_samp_by[iyr]*N_samp_by[iyr2] age_diff = iyr2-iyr HSP_prob = 4/N[iyr]*(phiA^age_diff) #nb: there&#39;s some duplication here! EC[iyr,iyr2]=M[iyr,iyr2]*HSP_prob } } list(EC=EC,M=M) } Let’s looks at a few scenarios, including (1) sampling ages proportional to their approximate abundance in the population, and (2) sampling ages biased towards younger age classes. We’ll sample 20 individuals per year, as might occur in a balanced monitoring program. We’ll generate an expected number of comparisons and an expected count for each combination of birth years for half-sib comparisons (omitting same-cohort comparisons). Npy = rep(0,20) Npy[11:20] = 20 # sampling proportional to what we&#39;d expect with a constant survival probability of 0.92 age_wts_prop = 0.92^c(0:5) #since &quot;ages&quot; is 3:8, this needs to consist of 6 weights age_wts_young = c(5:1) #5 times more likely to sample &amp; genotype a 3 year old than an 8 year old Exp_data_prop = expected_data(Npy,ylo,yhi,ages,age_wts=age_wts_prop,N,phiA) Exp_data_young = expected_data(Npy,ylo,yhi,ages,age_wts=age_wts_young,N,phiA) One thing we can look at right away is the number of kin pairs for the two designs. For the design with proportional age sampling the number of HSPs is 57.6215; for the design focusing disproportionally on younger ages, we have 101.9349 HSPs. Why might we have more HSPs when focusing on younger ages? I believe this is entirely because, on average, we are sampling individuals with birth years that are closer to each other, and potential parents will experience lower cumulative mortality between successive birth events. In fact, this has to be the case given that relative reproductive output is constant in this scenario (stable population) and the only thing that effects HSP probabilities that is different is survival! 3.4 Calculating expected variance under different design scenarios Now let’s fit a “correct” model to these data - that is, one in which all assumptions are met. Note that we don’t actually need to fit a model to do design calculations, but TMB computes variance estimates and delta method approximations for functions of parameters, so we’ll sacrifice a tiny bit of time estimating parameters in order to make our lives easier. If we were going to consider a large number of designs (or to do formal optimization of a design using quadratic programming or something) we’d want to revisit this decision!! So, let’s compile a TMB model, fit a model to data, and look at estimated standard errors of various quantities. # compile TMB negative log pseudo-likeihood function library(TMB) compile(&quot;TMB/hsp_nll2.cpp&quot;) ## [1] 0 # format data and specify starting values for parameters format_data &lt;- function(M,EC){ Indices_gt0 = which(M&gt;0,arr.ind=TRUE) Data = list( n_HSP = EC[Indices_gt0], n_UP = M[Indices_gt0]-EC[Indices_gt0], born_year = Indices_gt0[,2], #for this HSP calc only need birth year of the *younger* animal age_diff = Indices_gt0[,2]-Indices_gt0[,1], present_year= 20 #terminal year for abundance estimate ) Data } Data_prop = format_data(M=Exp_data_prop$M,EC=Exp_data_prop$EC) Data_young = format_data(Exp_data_young$M,Exp_data_young$EC) Parms = list(&quot;N_init&quot; = 1000, &quot;lambda&quot;=1.0, phiA=0.94) dyn.load(dynlib(&quot;TMB/hsp_nll2&quot;)) obj &lt;- TMB::MakeADFun(data = Data_prop, parameters = Parms, DLL=&quot;hsp_nll2&quot;) Opt = nlminb(start=Parms, objective=obj$fn, gradient=obj$gr) ## outer mgc: 698.1 ## Warning in nlminb(start = Parms, objective = obj$fn, ## gradient = obj$gr): NA/NaN function evaluation ## outer mgc: 852.9 ## outer mgc: 376.3 ## outer mgc: 156.6 ## outer mgc: 89.72 ## outer mgc: 4.769 ## outer mgc: 2.079 ## outer mgc: 0.04171 ## outer mgc: 0.03488 ## outer mgc: 0.002186 ## outer mgc: 0.0001492 SD_report_prop=sdreport(obj) ## outer mgc: 0.0001492 ## outer mgc: 0.0005512 ## outer mgc: 0.0008495 ## outer mgc: 8.839 ## outer mgc: 8.957 ## outer mgc: 2.981 ## outer mgc: 2.966 ## outer mgc: 20000 obj &lt;- TMB::MakeADFun(data = Data_young, parameters = Parms, DLL=&quot;hsp_nll2&quot;) Opt = nlminb(start=Parms, objective=obj$fn, gradient=obj$gr) ## outer mgc: 1241 ## Warning in nlminb(start = Parms, objective = obj$fn, ## gradient = obj$gr): NA/NaN function evaluation ## outer mgc: 1682 ## outer mgc: 716.8 ## outer mgc: 297.5 ## outer mgc: 209.9 ## outer mgc: 23.04 ## outer mgc: 32.68 ## outer mgc: 34.65 ## outer mgc: 3.625 ## outer mgc: 0.3037 ## outer mgc: 0.04553 ## outer mgc: 0.001343 SD_report_young=sdreport(obj) ## outer mgc: 0.001343 ## outer mgc: 0.002588 ## outer mgc: 9.75e-05 ## outer mgc: 15.84 ## outer mgc: 16.05 ## outer mgc: 5.448 ## outer mgc: 5.423 ## outer mgc: 20000 # Okay, let’s see what TMB did for us. First let’s check that our estimates are truth - they should be very close since we’re using expected values. print(SD_report_prop$value) ## N_init lambda phiA N_last ## 1.000e+03 -1.864e-08 9.400e-01 1.000e+03 print(SD_report_young$value) ## N_init lambda phiA N_last ## 1.000e+03 1.417e-07 9.400e-01 1.000e+03 Well that’s reassuring! How about standard errors? print(SD_report_prop$sd) ## [1] 616.47723 0.05528 0.05620 588.11186 print(SD_report_young$sd) ## [1] 452.05058 0.04014 0.04057 426.75313 So there is actually a large improvement in estimator performance (across all parameters) when sampling focuses on the youngest juvenile white sharks!! In fact, standard errors are 25-30% lower which is pretty substantial. This is because interbirth intervals are likely to be shorter, increasing the number of HSPs. Hopefully you can see why conducting this type of analysis is important - it will help to get the most bang for your buck in deciding what types of samples to target and genotype. References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
